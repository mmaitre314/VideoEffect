{"name":"Video Effects","tagline":"","body":"[![Build status](https://ci.appveyor.com/api/projects/status/vkkt3t5i4av2trs0?svg=true)](https://ci.appveyor.com/project/mmaitre314/videoeffect)\r\n[![NuGet package](http://mmaitre314.github.io/images/nuget.png)](https://www.nuget.org/packages/MMaitre.VideoEffects/)\r\n\r\nOriginal|Antique + HorizontalFlip\r\n----|----\r\n![Original](http://mmaitre314.github.io/VideoEffect/car_original.jpg)|![Processed](http://mmaitre314.github.io/VideoEffect/car_processed.jpg)\r\n\r\nApply image effects from the [Nokia Imaging SDK](http://developer.nokia.com/resources/library/Imaging_API_Ref/index.html) and [DirectX HLSL pixel shaders](http://msdn.microsoft.com/en-us/library/bb509635(v=VS.85).aspx) to videos in [Universal Store Apps](http://msdn.microsoft.com/en-us/library/windows/apps/dn609832.aspx) for Windows Phone 8.1 and Windows 8.1.\r\n\r\nEffects can be applied via [MediaTranscoder](http://msdn.microsoft.com/en-us/library/windows/apps/windows.media.transcoding.mediatranscoder.aspx), [MediaComposition](http://msdn.microsoft.com/en-us/library/windows/apps/xaml/windows.media.editing.mediacomposition.aspx), [MediaCapture](http://msdn.microsoft.com/en-us/library/windows/apps/xaml/windows.media.capture.mediacapture.aspx), or [MediaElement](http://msdn.microsoft.com/en-us/library/windows/apps/xaml/windows.ui.xaml.controls.mediaelement.aspx).\r\n\r\nBinaries are available via [NuGet](https://www.nuget.org/packages/MMaitre.VideoEffects/).\r\n\r\nNokia Imaging SDK effects\r\n-------------------------\r\n\r\nA video effect definition is created from a chain of image effects and passed to a video-processing class like MediaTranscoder:\r\n\r\n```c#\r\nvar definition = new LumiaEffectDefinition(() =>\r\n{\r\n    return new IFilter[]\r\n    {\r\n        new AntiqueFilter(),\r\n        new FlipFilter(FlipMode.Horizontal)\r\n    };\r\n});\r\n\r\nvar transcoder = new MediaTranscoder();\r\ntranscoder.AddVideoEffect(definition.ActivatableClassId, true, definition.Properties);\r\n```\r\n\r\n### Square videos\r\n\r\nImage effects changing the image resolution -- cropping for instance -- are also supported. In that case the resolutions of the input and output of the effect need to be specified explicitly. For instance, the following code snippet creates a square video:\r\n\r\n```c#\r\n// Select the largest centered square area in the input video\r\nvar encodingProfile = await TranscodingProfile.CreateFromFileAsync(file);\r\nuint inputWidth = encodingProfile.Video.Width;\r\nuint inputHeight = encodingProfile.Video.Height;\r\nuint outputLength = Math.Min(inputWidth, inputHeight);\r\nRect cropArea = new Rect(\r\n    (float)((inputWidth - outputLength) / 2),\r\n    (float)((inputHeight - outputLength) / 2),\r\n    (float)outputLength,\r\n    (float)outputLength\r\n);\r\n\r\nvar definition = new LumiaEffectDefinition(() =>\r\n{\r\n    return new IFilter[]\r\n    {\r\n        new CropFilter(cropArea)\r\n    };\r\n});\r\ndefinition.InputWidth = inputWidth;\r\ndefinition.InputHeight = inputHeight;\r\ndefinition.OutputWidth = outputLength;\r\ndefinition.OutputHeight = outputLength;\r\n```\r\n\r\nNote: in Windows Phone 8.1 a bug in MediaComposition prevents the width/height information to be properly passed to the effect.\r\n\r\n### Overlays\r\n\r\nBlendFilter can overlay an image on top of a video: \r\n\r\n```c#\r\nvar file = await StorageFile.GetFileFromApplicationUriAsync(new Uri(\"ms-appx:///Assets/traffic.png\"));\r\nvar foreground = new StorageFileImageSource(file);\r\nvar definition = new LumiaEffectDefinition(() =>\r\n{\r\n    var filter = new BlendFilter(foreground);\r\n    filter.TargetOutputOption = OutputOption.PreserveAspectRatio;\r\n    filter.TargetArea = new Rect(0, 0, .4, .4);\r\n    return new IFilter[] { filter };\r\n});\r\n```\r\n\r\n### Animations\r\n\r\nThe LumiaEffectDefinition() constructor is overloaded to support effects whose properties vary based on time. This requires creating a class implementing the IAnimatedFilterChain interface, with a 'Filters' property returning the current effect chain and an 'UpdateTime()' method receiving the current time. \r\n\r\n```c#\r\nclass AnimatedWarp : IAnimatedFilterChain\r\n{\r\n    WarpFilter _filter = new WarpFilter(WarpEffect.Twister, 0);\r\n\r\n    public IEnumerable<IFilter> Filters { get; private set; }\r\n\r\n    public void UpdateTime(TimeSpan time)\r\n    {\r\n        _filter.Level = .5 * (Math.Sin(2 * Math.PI * time.TotalSeconds) + 1); // 1Hz oscillation between 0 and 1\r\n    }\r\n\r\n    public AnimatedWarp()\r\n    {\r\n        Filters = new List<IFilter> { _filter };\r\n    }\r\n}\r\n\r\nvar definition = new LumiaEffectDefinition(() =>\r\n{\r\n    return new AnimatedWarp();\r\n});\r\n```\r\n\r\n### Bitmaps and pixel data\r\n\r\nFor cases where IFilter is not flexible enough, another overload of the LumiaEffectDefinition() constructor supports effects which handle Bitmap objects directly. This requires implementing IBitmapVideoEffect, which has a single Process() method called with an input bitmap, an output bitmap, and the current time for each frame in the video.\r\n\r\nThe bitmaps passed to the Process() call get destroyed when the call returns, so any async call in this method must be executed synchronously using '.AsTask().Wait()'.\r\n\r\nThe following code snippet shows how to apply a watercolor effect to the video:\r\n\r\n```c#\r\nclass WatercolorEffect : IBitmapVideoEffect\r\n{\r\n    public void Process(Bitmap input, Bitmap output, TimeSpan time)\r\n    {\r\n        var effect = new FilterEffect();\r\n        effect.Filters = new IFilter[]{ new WatercolorFilter() };\r\n        effect.Source = new BitmapImageSource(input);\r\n        var renderer = new BitmapRenderer(effect, output);\r\n        renderer.RenderAsync().AsTask().Wait(); // Async calls must run sync inside Process()\r\n    }\r\n}\r\n\r\nvar definition = new LumiaEffectDefinition(() =>\r\n{\r\n    return new WatercolorEffect();\r\n});\r\n```\r\n\r\nIBitmapVideoEffect also allows raw pixel-data processing. Pixel data is provided in Bgra888 format (32 bits per pixel). The content of the alpha channel is undefined and should not be used.\r\n\r\nFor efficiency, a GetData() method extension is provided on IBuffer. It is enabled by adding a 'using VideoEffectExtensions;' statement. GetData() returns an 'unsafe' byte* pointing to the IBuffer data. This requires methods calling GetData() to be marked using the 'unsafe' keyword and to check the 'Allow unsafe code' checkbox in the project build properties.\r\n\r\nThe following code snippet shows how to apply a blue filter by setting both the red and green channels to zero:\r\n\r\n```c#\r\nusing VideoEffectExtensions;\r\n\r\nclass BlueEffect : IBitmapVideoEffect\r\n{\r\n    public unsafe void Process(Bitmap input, Bitmap output, TimeSpan time)\r\n    {\r\n        uint width = (uint)input.Dimensions.Width;\r\n        uint height = (uint)input.Dimensions.Height;\r\n\r\n        uint  inputPitch = input.Buffers[0].Pitch;\r\n        byte* inputData  = input.Buffers[0].Buffer.GetData();\r\n        uint  outputPitch = output.Buffers[0].Pitch;\r\n        byte* outputData  = output.Buffers[0].Buffer.GetData();\r\n\r\n        for (uint i = 0; i < height; i++)\r\n        {\r\n            for (uint j = 0; j < width; j++)\r\n            {\r\n                outputData[i * outputPitch + 4 * j + 0] = inputData[i * inputPitch + 4 * j + 0]; // B\r\n                outputData[i * outputPitch + 4 * j + 1] = 0; // G\r\n                outputData[i * outputPitch + 4 * j + 2] = 0; // R\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nRaw pixel data tends to be the least common denominator of image-processing libraries. It can be used for instance to detect QR codes and barcodes using the [ZXing.Net](http://www.nuget.org/packages/ZXing.Net/) library:\r\n\r\n```c#\r\nusing System.Runtime.InteropServices.WindowsRuntime;\r\n\r\nclass QrCodeDetector : IBitmapVideoEffect\r\n{\r\n    BarcodeReader m_reader = new BarcodeReader\r\n    {\r\n        PossibleFormats = new BarcodeFormat[] { BarcodeFormat.QR_CODE }\r\n    };\r\n\r\n    public void Process(Bitmap input, Bitmap output, TimeSpan time)\r\n    {\r\n        // Pass-through effect\r\n        output.CopyDataFrom(input);\r\n\r\n        Result result = m_reader.Decode(\r\n            input.Buffers[0].Buffer.ToArray(), \r\n            (int)input.Dimensions.Width, \r\n            (int)input.Dimensions.Height, \r\n            BitmapFormat.BGR32\r\n            );\r\n\r\n        Debug.WriteLine(\"Result: {0}\", result == null ? \"<none>\" : result.Text);\r\n    }\r\n}\r\n```\r\n\r\nThe code snippet above is fairly inefficient though (unnecessary copies/conversions, etc.) and could be improved, so if realtime detection is a scenario of interest let me know.\r\n\r\nDirectX HLSL Pixel Shader effects\r\n---------------------------------\r\n\r\nEffects can process videos in either Bgra8 or Nv12 color spaces. Processing Bgra8 is simpler (one shader to write instead of two) but less efficient (the video pipeline needs to add one or two color conversions from/to Nv12/Yuy2).\r\n\r\nIn the case of Nv12, the luma (Y) and chroma (UV) color planes are generated separately. For instance, for a basic color-inversion effect:\r\n\r\n```hlsl\r\n// Y processing\r\nfloat4 main(Pixel pixel) : SV_Target\r\n{\r\n    float y = bufferY.Sample(ss, pixel.pos);\r\n    y = 1 - y;\r\n    return y;\r\n}\r\n\r\n// UV processing\r\nfloat4 main(Pixel pixel) : SV_Target\r\n{\r\n    float4 uv = bufferUV.Sample(ss, pixel.pos);\r\n    uv = 1 - uv;\r\n    return uv;\r\n}\r\n```\r\n\r\nVisual Studio compiles the shaders into .cso files which are included in the app package and loaded at runtime to create a video effect definition:\r\n\r\n```c#\r\n    IBuffer shaderY = await PathIO.ReadBufferAsync(\"ms-appx:///Invert_093_NV12_Y.cso\");\r\n    IBuffer shaderUV = await PathIO.ReadBufferAsync(\"ms-appx:///Invert_093_NV12_UV.cso\");\r\n    var definition = new ShaderEffectDefinitionNv12(shaderY, shaderUV);\r\n\r\n    var transcoder = new MediaTranscoder();\r\n    transcoder.AddVideoEffect(definition.ActivatableClassId, true, definition.Properties);\r\n```\r\n\r\nFor effects to run on Windows Phone 8.1, in the file property page 'Configuration Properties > HLSL Compiler > General > Shader Model' must be set to 'Shader Model 4 Level 9_3 (/4_0_level_9_3)'. Visual Studio only supports compiling shaders in C++ project, so for C# app a separate C++ project should be created to compile the shaders.\r\n\r\nFor the .cso files to be included in the app package, in their file property page 'Build Action' must be set to 'Content'.\r\n\r\nImplementation details\r\n----------------------\r\n\r\nThe meat of the code is under VideoEffects/VideoEffects/VideoEffects.Shared. It consists in three Windows Runtime Classes: VideoEffects.LumiaEffect,  VideoEffects.ShaderEffectNv12, and VideoEffects.ShaderEffectBgrx8. LumiaEffect wraps a chain of Imaging SDKâ€™s IFilter inside [IMFTransform](http://msdn.microsoft.com/en-us/library/windows/desktop/ms696260)/[IMediaExtension](http://msdn.microsoft.com/en-us/library/windows/apps/windows.media.imediaextension.aspx). ShaderEffectXxx wraps a precompiled DirectX HSLS pixel shader. The rest is mostly support code and unit tests. \r\n\r\nThe Runtime Classes must be declared in the AppxManifest files of Store apps wanting to call it:\r\n\r\n```xml\r\n<Extensions>\r\n  <Extension Category=\"windows.activatableClass.inProcessServer\">\r\n    <InProcessServer>\r\n      <Path>VideoEffects.WindowsPhone.dll</Path>\r\n      <ActivatableClass ActivatableClassId=\"VideoEffects.LumiaEffect\" ThreadingModel=\"both\" />\r\n        <ActivatableClass ActivatableClassId=\"VideoEffects.ShaderEffectBgrx8\" ThreadingModel=\"both\" />\r\n        <ActivatableClass ActivatableClassId=\"VideoEffects.ShaderEffectNv12\" ThreadingModel=\"both\" />\r\n    </InProcessServer>\r\n  </Extension>\r\n</Extensions>\r\n```\r\n\r\nVisual Studio does not handle such an `<Extension>` element. The AppxManifest needs to be opened as raw XML and the XML code snippet above copy/pasted. For Windows Store apps the `<path>` is VideoEffects.Windows.dll. NuGet packages handle that part automatically when targeting C# Store apps.\r\n\r\nVideo frames are received as [IMF2DBuffer2](http://msdn.microsoft.com/en-us/library/windows/desktop/hh447827) from the Media Foundation pipeline and successively wrapped inside [IBuffer](http://msdn.microsoft.com/en-us/library/windows/apps/windows.storage.streams.ibuffer.aspx), Bitmap, and BitmapImageSource/BitmapRenderer to be handed to the Nokia Imaging SDK.\r\n","google":"UA-55657833-1","note":"Don't delete this file! It's used internally to help with page regeneration."}